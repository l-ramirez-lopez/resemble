---
title: "Modelling complex spectral data with the resemble package"
author: 
 - name: Leonardo Ramirez-Lopez, Alexandre M.J.-C. Wadoux, Raphael Viscarra-Rossel
   email: ramirez.lopez.leo@gmail.com
date: "`r Sys.Date()`"
bibliography: ["one.bib"]
biblio-style: "apalike"
link-citations: true
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Modelling complex spectral data with the resemble package}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
library(formatR)
knitr::opts_chunk$set(
  collapse = TRUE
)
```

<em><p align="right"> Think Globally, Fit Locally [@saul2003think] </p></em>


# Introduction

Modeling spectral data has garnered wide interest in the last four decades. 
Spectroscopy is the study of the spectral response of a matrix (e.g. soil, 
plant material, seeds, etc.) when it interacts with electromagnetic radiation. 
This spectral response directly or indirectly relates to a wide range of 
compositional characteristics (chemical, physical or biological) of the matrix. 
Therefore, it is possible to develop empirical models that can accurately 
quantify properties of different matrices. In this respect, quantitative 
spectroscopy techniques are usually fast, non-destructive and cost-efficient in 
comparison to conventional laboratory methods used in the analyses of these 
matrices.  This has resulted in the development of comprehensive 
spectral databases for several agricultural products comprising large amounts 
of observations. The size of such databases increases *de facto* their 
complexity. To analyze large and complex spectral data, one must then resort 
numerical and statistical tools such as dimensionality reduction, and local 
spectroscopic modelling based on spectral dissimilarity concepts.    

The aim of the `resemble` package is to provide tools to efficiently and 
accurately extract meaningful quantitative information from large and complex 
spectral databases. The package contains functions for dimensionality reduction, 
spectral dissimilarity measurements, neighbour search, and local modeling. 
The core functionalities of the package include: 

* dimensionality reduction
* computation of dissimilarity measures
* evaluation of dissimilarity matrices
* spectral neighbour search
* fitting and predicting local spectroscopic models

# Example dataset

This vignette uses a soil spectroscopic dataset provided in the `prospectr`
package [@stevens2020introduction]. It is a soil spectral library used in the 
\sQuote{Chimiometrie 2006} challenge by @pierna2008soil. The library contains 
absorbance spectra of dried and sieved 825 soil observations/samples. These 
samples originate from agricultural fields collected from all over the Walloon 
region in Belgium. The dataset is in a data frame which is organized as follows:
 
* __Response variables__: 
  * ___Nt___ (Total Nitrogen in g/Kg of dry soil): A numerical variable (values 
      are available for 645 samples and missing for 180 samples).
  * ___Ciso___ (Carbon in g/100 g of dry soil): A numerical 
      variable (values are available for 732 and missing for 93 samples).
  * ___CEC___ (Cation Exchange Capacity in meq/100 g of dry soil): A numerical 
      variable (values are available for 447 and missing for 378 samples).
* __Predictor variables__: The predictor variables are in a matrix embedded in the
data frame, which can be accessed via `NIRSoil$spc`. These variables contain the
absorbance Near-Infrared (NIR) spectra of the samples recorded between the 
1100 nm and 2498 nm of the electromagnetic spectrum at 2 nm interval. Each 
column name in the matrix of spectra represent a specific wavelength (in nm).
* __Set__: this is a "binary" variable that indicates the 618 samples belong to the 
training subset (represented by 1) and the 207 samples that belong to the test 
subset (represented by 0). 

Load the necessary packages and data. 
```{r libraries, tidy = TRUE, message = FALSE}
library(resemble)
library(prospectr)
library(magrittr)
```

The dataset can be loaded into R as follows: 
```{r, tidy = FALSE, message = FALSE, results = 'hide'}
data(NIRsoil)
dim(NIRsoil)
str(NIRsoil)
```

# Spectra pre-processing
This step aims at improving the signal quality of the spectra for quantitative
analysis. In this respect, the following standard methods are applied using the 
package `prospectr` [@stevens2020introduction]: 

1. Scatter correction using the standard normal variate method
[@barnes1989standard].
2. First derivative using Savitsky-Golay filtering [@Savitzky1964]. 

```{r NIRsoil, tidy = FALSE, message = FALSE}
# obtain a numeric vector of the wavelengths at which spectra is recorded 
wavs <- NIRsoil$spc %>% colnames() %>% as.numeric()

# pre-process the spectra
poly_order <- 1
window <- 5
diff_order <- 1

NIRsoil$spc_p <- NIRsoil$spc %>% 
  standardNormalVariate() %>% 
  savitzkyGolay(p = poly_order, w = window, m = diff_order)
```

For more explicit examples, the `NIRSoil` data is split into training and 
testing subsets: 

```{r}
# training dataset
training  <- NIRsoil[NIRsoil$train == 1, ]
testing  <- NIRsoil[NIRsoil$train == 0, ]
```

Note that in the resemble package we follow the notation provided by 
@ramirez2013spectrum, i.e.: 

* __Training observations__:

  * `Xr` stands for the matrix of predictor variables in the reference/training 
set (spectral data for calibration).

  * `Yr` stands for the response variable(s) in the reference/training set 
(dependent variable for calibration).In the context of this package, `Yr` is 
also referred as to "___side information___", which is a variable or set of 
variables that are associated to the training observations that can also be used 
to support or guide optimization during modeling but that not necessarily are 
part of the input of such models. For example, we will see in latter sections 
that `Yr` can be used in Principal Component Analysis to help to decide how 
many components are optimal.


* __Testing observations__:

  * `Xu` stands for the matrix of predictor variables in the unknown/test 
set (spectral data for validation/esting).

  * `Yu` stands for the response variable(s) in the unknown/test set (dependent 
variable for calibration).

# Dimensionality reduction
When conducting exploratory analysis of spectral data, we are immediately 
burdened with the issue of high dimensionality. It is such that we may be 
dealing with (using NIR spectra data as an example) hundreds to thousands of 
individual wavelengths for each spectrum. When one wants to investigate patterns 
in the data, spectral similarities and differences, or detect spectral outliers, 
it is necessary to reduce the dimension of the spectra while retaining important 
information. 

Principal component (PC) analysis and Partial Least Squares (PLS) decomposition 
methods assume that the meaningful structure the data intrinsically lies on 
a lower dimensional space. Both methods attempt to find a projection matrix 
that projects or converts the original variables onto a new and less complex 
space represented by few variables. These new variables mimic the original 
variability across observations. These two methods can be considered as the 
standard ones for dimensionality reduction in many fields of spectroscopic 
analysis. 

The difference between PC and PLS is that in the first the objective is to 
find few new variables (which are orthogonal) that capture as much of the 
original data variance while in the latter the objective is to find few new 
variables that maximize their variance with respect to a set of one or more 
external variables (e.g. response variables or side information variables). 

In the `resemble` package PC analysis and PLS decomposition are available 
through the `ortho_projection()` function which offers the following algorithms:

* `"pca"`: the standard method for PC analysis based on the singular value 
decomposition algorithm.

* `"pca.nipals"`: this algorithm uses the non-linear iterative partial 
least squares algorithm [NIPALS, @wold1975soft] for the purpose of PC analysis.

* `"pls"`: Here, PLS decomposition also uses the NIPALS algorithm, but in this 
case it makes use of ___side information___, which can be a variable or set of 
variables that are associated to the training observations and that are used to
project the data. In this case, the variance between the projected variables and 
the ___side information___ variable(s) is maximized. 

The PC analysis of the training spectra can be executed as follows:
```{r, results = 'hide'}
# principal component (pc) analysis with the default 
# method (singular value decomposition, SVD) for 5 components
# principal component (pc) analysis with the default 
# method (singular value decomposition) 
pca_tr <- ortho_projection(Xr = training$spc_p,
                           method = "pca")

pca_tr
```

plot the `ortho_projection` object:

```{r fig.cap = "Cumulative explained variance of the principal components (left) and individual contribution the the explained variance for each of the components (right).", fig.id = "plot_pcs", fig.cap.style = "Image Caption", fig.align = "center", fig.width = 8, fig.height = 3}
plot(pca_tr)
```

The code above shows that in this dataset, `r pca_tr$n_components` components 
are required to explain around `r round(100 * sum(pca_tr$variance[2,]), 0)`% of 
the original variance found in the spectra.

Equivalent results can be obtained by using the NIPALS algorithm:
```{r, results = 'hide'}
# principal component (pc) analysis with the default 
# method (singular value decomposition) 
pca_nipals_tr <- ortho_projection(Xr = training$spc_p,
                                  method = "pca.nipals")

pca_nipals_tr
```

The advantage of the NIPALS algorithm is that it can be faster than SVD when 
only few components are required.

For a PLS decomposition the `method` argument is set to `"pls"`. In this case, 
side information (`Yr`) is required. In the following example, the side 
information used is the Total Carbon (`Ciso`):
```{r, results = 'hide', eval = FALSE}
# Partial Least Squares decomposition using 
# Total carbon as side information
# (this might take some seconds)
pls_tr <- ortho_projection(Xr = training$spc_p[!is.na(training$Ciso),],
                           Yr = training$Ciso[!is.na(training$Ciso)],
                           method = "pls")
pls_tr
```

In the above code, the PLS method retrieves more components than the PC methods.
This is due to the default optimization of the number of components in the 
function. By default in `ortho_projection` the retained components should 
account for at least 99% of the original variance of data. 


##################
MODIFY THE CODE SO THAT method = "cumvar" ACCOUNTS FOR AT LEAST AND NOT FOR MAXIMUM???






based on the total 
amount of 99% of original variance that the retained components should account for. 

components that account for a cumulative explained variance of 99%



The results above also show that four components only are sufficient to explain 
nearly 88% of the variance of the original dataset. For the example, we may also 
want to use a user-defined amount of cumulative variance explained that needs 
to be retained. Usually, we expect the PCs to explain at least 90% of the 
variance of the original data. For spectral data with large variability, this 
means that we may end up for more than simply five components in total. 

Principal component analysis using a user defined values of the cumulative 
variance to be explained. We need to set `"var"` in the argument `pc_selection`. 












The argument `pc_selection` of the `ortho_projection()` function helps in the 
selection of the number of components/dimensions the spectra should be 
reduced to. The following options are available:

* Manual selection, `"manual"`: the user explicitly defines how many 
components to retrieve.

* Cumulative variance-based selection, `"cumvar"`: only the first 
components that together explain an user-defined amount of the original spectral 
variance are retained. 

* Single component explanined variance-based selection, `"var"`: those 
components that alone explain more than an user-defined amount of the original 
spectral variance are retained. 

* Optimal component selection `"opc"`: the selection of the components is 
carried out by using an iterative method based on the side information concept
presented in @ramirez2013spectrum. First let be $P$ a sequence of retained 
components (so that $P = 1, 2, ...,k$). At each iteration, the function computes 
a dissimilarity matrix retaining $p_i$ components. The values in this side 
information variable are compared against the side information values of their 
most spectrally similar observations.
The optimal number of components retrieved by the function is the one that 
minimizes the root mean squared differences (RMSD) in the case of continuous
variables, or maximizes the kappa index in the case of categorical variables.
In this process, the `sim_eval` function is used. Note that for the `"opc"` 
method `Yr` is required (i.e. the side information of the observations).

We describe demo the above options case in the following subsections. 


## Principal components (PC) analysis
This method has become the standard for dimensionality reduction. In this method, 
the goal is to mimic the original variability across observations in the data 
but with (a new set of) fewer and uncorrelated variables called called principal 
components. PC analysis attempts to remove all the redundant variable information 
in the data (i.e. collinearity) by projecting the original variables onto new 
ones that summarize them. The first PC accounts for a large amount 
of the variability in the original data, and each succeeding component accounts 
for a sequentially decreasing amount of the remaining variability. Therefore, 
most of the original variability can be captured in the first few components. 
By removing collinearity and reducing dimensionality, the complexity of the data 
is then reduced, which allows better performance in tasks such as outlier 
detection, sample dissimilarity analysis, representative sample analysis etc. 

In this package, the `ortho_projection()` function can be used for PC analysis. 
Let's use the `ortho_projection()` function to compute the first 5 PCs (arbitrary 
chosen) of the preprocessed spectra in the training set using the default 
method which is the singular value decomposition (SVD) algorithm:











```{r}
# specify amount of maximum cumulative variance that needs to be retained by 
# the PCs 
threshold_expl_var <- 0.99

pcs_tr_expl_var <- ortho_projection(Xr = training$spc_p,
                                    pc_selection = list("cumvar", threshold_expl_var),
                                    method = "pca")
pcs_tr_expl_var 
```








A more sophisticated option to optimize the number is also a


## References

