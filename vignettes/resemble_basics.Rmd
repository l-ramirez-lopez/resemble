---
title: "Modelling complex spectral data with the resemble package"
author: 
 - name: Leonardo Ramirez-Lopez, Alexandre M.J.-C. Wadoux, Raphael Viscarra-Rossel
   email: ramirez.lopez.leo@gmail.com
date: "`r Sys.Date()`"
bibliography: ["one.bib"]
biblio-style: "apalike"
link-citations: true
output: knitr:::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Bulding highly accurate local models with the resemble package}
---

```{r setup, include = FALSE}
library(formatR)
knitr::opts_chunk$set(
  collapse = TRUE
)
```

<em><p align="right"> Think Globally, Fit Locally [@saul2003think] </p></em>


# Introduction

Modeling spectral data has garnered wide interest in the last four decades. 
Spectroscopy is the study of the spectral response of a matrix (e.g. soil, 
plant material, seeds, etc.) when it interacts with electromagnetic radiation. 
This spectral response directly or indirectly relates to a wide range of 
compositional characteristics (chemical, physical or biological) of the matrix. 
Therefore, it is possible to develop empirical models that can accurately 
quantify properties of different matrices. In this respect, quantitative 
spectroscopy techniques are usually fast, non-destructive and cost-efficient in 
comparison to conventional laboratory methods used in the analyses of these 
matrices.  This has resulted in the development of comprehensive 
spectral databases for several agricultural products comprising large amounts 
of observations. The size of such databases increases *de facto* their 
complexity. To analyze large and complex spectral data, one must then resort 
numerical and statistical tools such as dimensionality reduction, and local 
spectroscopic modelling based on spectral dissimilarity concepts.    

The aim of the `resemble` package is to provide tools to efficiently and 
accurately extract meaningful quantitative information from large and complex 
spectral databases. The package contains functions for dimensionality reduction, 
spectral dissimilarity measurements, neighbour search, and local modeling. 
The core functionalities of the package include: 

* dimensionality reduction
* computation of dissimilarity measures
* evaluation of dissimilarity matrices
* spectral neighbour search
* fitting and predicting local spectroscopic models

# Example dataset

This vignette uses a soil spectroscopic dataset provided in the `prospectr`
package. It is a soil spectral library used in the \sQuote{Chimiometrie 2006} 
challenge by @pierna2008soil. The library contains absorbance spectra of dried 
and sieved 825 soil observations/samples. These samples originate from agricultural 
fields collected from all over the Walloon region in Belgium. The dataset is 
in a data frame which is organized as follows:
 
* Response variables: 
  * Nt (Total Nitrogen in g/Kg of dry soil): A numerical variable (values 
      are available for 645 samples and missing for 180 samples).
  * Ciso (Carbon in g/100 g of dry soil): A numerical 
      variable (values are available for 732 and missing for 93 samples).
  * CEC (Cation Exchange Capacity in meq/100 g of dry soil): A numerical 
      variable (values are available for 447 and missing for 378 samples).
* Predictor variables: The predictor variables are in a matrix embedded in the
data frame, which can be accessed via `NIRSoil$spc`. These variables contain the
absorbance Near-Infrared (NIR) spectra of the samples recorded between the 
1100 nm and 2498 nm of the electromagnetic spectrum at 2 nm interval. Each 
column name in the matrix of spectra represent a specific wavelength (in nm).
* Set: this is a "binary" variable that indicates the 618 samples belong to the 
training subset (represented by 1) and the 207 samples that belong to the test 
subset (represented by 0). 

Load the necessary packages and data. 
```{r libraries, tidy = TRUE, message = FALSE}
library(resemble)
library(prospectr)
library(magrittr)
```

The dataset can be loaded into R as follows: 
```{r, tidy = FALSE, message = FALSE}
data(NIRsoil)
dim(NIRsoil)
str(NIRsoil)
```

# Spectra pre-processing
This step aims at improving the signal quality of the spectra for quantitative
analysis. In this respect, the following standard methods are applied: 

1. Scatter correction using the standard normal variate method
[@barnes1989standard] implemented in the `prospectr` package.
2. First derivative using Savitsky-Golay [@Savitzky1964] filtering also 
implemented in the `prospectr` package. 

```{r NIRsoil, tidy = FALSE, message = FALSE}
# obtain a numeric vector of the wavelengths at which spectra is recorded 
wavs <- NIRsoil$spc %>% colnames() %>% as.numeric()

# pre-process the spectra
poly_order <- 1
window <- 5
diff_order <- 1

NIRsoil$spc_p <- NIRsoil$spc %>% 
  standardNormalVariate() %>% 
  savitzkyGolay(p = poly_order, w = window, m = diff_order)
```

For more explicit examples, the `NIRSoil` data is split into training and 
testing subsets: 

```{r}
# training dataset
training  <- NIRsoil[NIRsoil$train == 1, ]
testing  <- NIRsoil[NIRsoil$train == 0, ]
```

Note that in the resemble package we follow the notation provided by 
@ramirez2013spectrum, i.e.: 

* `Xr` stands for the matrix of predictor variables in the reference/training 
set (spectral data for calibration).
* `Yr` stands for the response variable(s) in the reference/training set 
(dependent variable for calibration).
* `Xu` stands for the matrix of predictor variables in the unknown/test 
set (spectral data for validation/esting).
* `Yu` stands for the response variable(s) in the unknown/test set (dependent 
variable for calibration).

# Dimensionality reduction
When conducting exploratory analysis of spectral data, we are immediately 
burdened with the issue of high dimensionality. It is such that we may be 
dealing with (using NIR spectra data as an example) hundreds to thousands of 
individual wavelengths for each spectrum. When one wants to investigate patterns 
in the data, spectral similarities and differences, or detect spectral outliers, 
it is necessary to reduce the dimension of the spectra while retaining important 
information. 

Principal component (PC) analysis and Partial least squares (PLS) methods can be 
considered as the standards for dimensionality reduction in many fields of 
spectroscopic analysis. In both methods, the goal is to mimic the original 
variability across observations in the data but with (a new set of) fewer and 
uncorrelated variables. This is done by finding a projection matrix that projects
or converts the original variables onto the new and less complex variable space. 

The difference between PC and PLS is that the first projects the new variables in 
the direction of their maximal variation while the latter projects the new 
variables in a way that maximizes the variance between the new variables and a 
set consisting of one or more external variables (e.g. response variables or side 
information variables).

PC analysis attempts to remove all the redundant variable information 
in the data (i.e. collinearity) by projecting the original variables onto new 
ones that summarize them. The first PC accounts for a large amount 
of the variability in the original data, and each succeeding component accounts 
for a sequentially decreasing amount of the remaining variability. Therefore, 
most of the original variability can be captured in the first few components. 
By removing collinearity and reducing dimensionality, the complexity of the data 
is then reduced, which allows better performance in tasks such as outlier 
detection, sample dissimilarity analysis, representative sample analysis etc. 






Several functions are available in the `resemble` package for dimensionality 
reduction. The master function `ortho_projection()` is a wrapper for two other 
functions: `pc_projection()` and `pls_projection()`. The function offer several 
options for dimensionality reduction using different methods for projecting the 
data:

* `"pca"`: the standard method for PCA based on the singular value decomposition 
algorithm.

* `"pca.nipals"`: PCA based on the non-linear iterative partial least squares 
algorithm.

* `"pls"`: partial least squares algorithm for projection, makes use of side 
information which is indeed equivalent response variable information. In this
case, the side information data must be passed to the `Yr` argument.













The argument `pc_selection` of the `ortho_projection()` function helps in the 
selection of the number of components/dimensions the spectra should be 
reduced to. The following options are available:

* Manual selection, `"manual"`: the user explicitly defines how many 
components to retrieve.

* Cumulative variance-based selection, `"cumvar"`: only the first 
components that together explain an user-defined amount of the original spectral 
variance are retained. 

* Single component explanined variance-based selection, `"var"`: those 
components that alone explain more than an user-defined amount of the original 
spectral variance are retained. 

* Optimal component selection `"opc"`: the selection of the components is 
carried out by using an iterative method based on the side information concept
presented in @ramirez2013spectrum. First let be $P$ a sequence of retained 
components (so that $P = 1, 2, ...,k$). At each iteration, the function computes 
a dissimilarity matrix retaining $p_i$ components. The values in this side 
information variable are compared against the side information values of their 
most spectrally similar observations.
The optimal number of components retrieved by the function is the one that 
minimizes the root mean squared differences (RMSD) in the case of continuous
variables, or maximizes the kappa index in the case of categorical variables.
In this process, the `sim_eval` function is used. Note that for the `"opc"` 
method `Yr` is required (i.e. the side information of the observations).

We describe demo the above options case in the following subsections. 


## Principal components (PC) analysis
This method has become the standard for dimensionality reduction. In this method, 
the goal is to mimic the original variability across observations in the data 
but with (a new set of) fewer and uncorrelated variables called called principal 
components. PC analysis attempts to remove all the redundant variable information 
in the data (i.e. collinearity) by projecting the original variables onto new 
ones that summarize them. The first PC accounts for a large amount 
of the variability in the original data, and each succeeding component accounts 
for a sequentially decreasing amount of the remaining variability. Therefore, 
most of the original variability can be captured in the first few components. 
By removing collinearity and reducing dimensionality, the complexity of the data 
is then reduced, which allows better performance in tasks such as outlier 
detection, sample dissimilarity analysis, representative sample analysis etc. 

In this package, the `ortho_projection()` function can be used for PC analysis. 
Let's use the `ortho_projection()` function to compute the first 5 PCs (arbitrary 
chosen) of the preprocessed spectra in the training set using the default 
method which is the singular value decomposition (SVD) algorithm:

```{r}
# principal component (pc) analysis with the default 
# method (singular value decomposition, SVD) for 5 components
pcs_tr_manual <- ortho_projection(Xr = training$spc_p,
                                  pc_selection = list("manual", 5),
                                  method = "pca")

pcs_tr_manual
```

The same results can be obtained by using the "non-linear iterative partial 
least squares (nipals) algorithm:

```{r}
# principal component (pc) analysis using pca nipals for 5 components
five_pcs_tr_nipals <- ortho_projection(Xr = training$spc_p,
                                       pc_selection = list("manual", 5),
                                       method = "pca.nipals")

five_pcs_tr_nipals
```

The advantage of the nipals algorithm is that it can be faster than SVD when 
only components are required.

The results above also show that four components only are sufficient to explain 
nearly 88% of the variance of the original dataset. For the example, we may also 
want to use a user-defined amount of cumulative variance explained that needs 
to be retained. Usually, we expect the PCs to explain at least 90% of the 
variance of the original data. For spectral data with large variability, this 
means that we may end up for more than simply five components in total. 

Principal component analysis using a user defined values of the cumulative 
variance to be explained. We need to set `"var"` in the argument `pc_selection`. 

```{r}
# specify amount of maximum cumulative variance that needs to be retained by 
# the PCs 
threshold_expl_var <- 0.99

pcs_tr_expl_var <- ortho_projection(Xr = training$spc_p,
                                    pc_selection = list("cumvar", threshold_expl_var),
                                    method = "pca")
pcs_tr_expl_var 
```

```{r, fig.align='center', fig.width = 8, fig.height = 3, fig.cap = 'Cumulative explained variance of the principal components (left) and individual contribution the the explained variance for each of the components (right).'}
o_mfrow <- par()$mfrow
par(mfrow = c(1, 2))
plot(pcs_tr_expl_var)
par(mfrow = o_mfrow)
```

The code above shows that in this dataset, 15 components are required to explain 
a maximum of 99% of the original variance found in the spectra.

A more sophisticated option to optimize the number is also a


## References

